{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZQ2c1Cv4vPM75wWGlVwYo"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "-YrU50Xnj0-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-UDF_1iD6Ai",
        "outputId": "02bba39d-ed57-4299-8aed-1676c25e7970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.0+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyg_lib torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
        "!pip install torch-geometric\n",
        "!pip install torch-geometric-temporal\n"
      ],
      "metadata": {
        "id": "qpcFXivDYGcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GCNConv\n",
        "import sys\n",
        "import torch_geometric\n",
        "from torch_geometric_temporal.nn.recurrent.temporalgcn import TGCN\n",
        "from torch_geometric_temporal.nn.recurrent.temporalgcn import TGCN2\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n"
      ],
      "metadata": {
        "id": "fg6uA2ElX-1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TGCNMODEL(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels,hidden_channel,emb, num_nodes, num_layers, dropout):\n",
        "        super(TGCNMODEL, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.hidden_channel=hidden_channel\n",
        "        self.num_nodes = num_nodes\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # define the temporal convolutional layers\n",
        "        self.temp_conv_layers = nn.ModuleList()\n",
        "        self.temp_conv_layers.append(nn.Conv2d(in_channels, hidden_channel, kernel_size=(1, 3)))\n",
        "        self.temp_conv_layers.append(nn.Conv2d(hidden_channel, out_channels, kernel_size=(1, 3)))\n",
        "\n",
        "        # define the attention mechanism\n",
        "        self.attention = nn.MultiheadAttention(emb, num_heads=2, dropout=dropout,batch_first=True)\n",
        "\n",
        "        # define the dropout layer\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "\n",
        "        # define the output layer\n",
        "        self.output_layer = nn.Linear(emb, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x, adj_matrices):\n",
        "        x=x.permute(0,3,1,2)\n",
        "        # x has shape (batch_size, seq_length, num_nodes, in_channels)\n",
        "        # adj_matrices has shape (batch_size, num_nodes, num_nodes)\n",
        "\n",
        "\n",
        "        # apply the temporal convolutional layers\n",
        "        h = x\n",
        "        for i in range(self.num_layers):\n",
        "            # apply the convolution\n",
        "            h = self.temp_conv_layers[i](h)\n",
        "\n",
        "            # apply the adjacency matrix\n",
        "            batch_size = h.size(0)\n",
        "            adj_matrices_i = adj_matrices.unsqueeze(dim=1).repeat(1, h.size(1), 1, 1)\n",
        "            h=torch.einsum('bsnj,bsjc->bsnc', (adj_matrices_i,h))\n",
        "            # h = torch.einsum('bsnc,bnc->bsnl', (adj_matrices_i,h))\n",
        "\n",
        "            # apply the activation function\n",
        "            h = F.relu(h)\n",
        "\n",
        "            # apply dropout\n",
        "            h = self.dropout_layer(h)\n",
        "\n",
        "        # compute the self-attention\n",
        "        h = h.permute(0, 2, 1, 3)# shape (batch_size, seq_length, num_nodes, out_channels), batch_size-node-sequence-feature\n",
        "        dim3=h.shape[2]\n",
        "        dim4=h.shape[3]\n",
        "        dim1=h.shape[0]\n",
        "        h = h.reshape(-1, dim3, dim4) # shape (batch_size * num_nodes,seq_length, out_channels)\n",
        "        h = self.attention (h, h, h)[0]\n",
        "        h = h.reshape(dim1, self.num_nodes, dim3, dim4).permute(0, 2, 1, 3)\n",
        "\n",
        "        # compute the output\n",
        "        h = h.mean(dim=1) # take the mean over the sequence dimension\n",
        "        # h = self.output_layer(h).view(32, -1, 26)\n",
        "        h = self.output_layer(h).squeeze(dim=-1) #.squeeze(dim=-2) # shape (batch_size, num_nodes)\n",
        "\n",
        "        return h\n"
      ],
      "metadata": {
        "id": "7o4ue9cFwfyA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
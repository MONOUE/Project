{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM36ipAkCt2CBHqoRLA4OPW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MONOUE/Project/blob/main/utils_func.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tslearn"
      ],
      "metadata": {
        "id": "hFh9s9uwruLE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e547a036-e4e8-4833-eb4c-f3e26accd298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tslearn in /usr/local/lib/python3.10/dist-packages (0.5.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tslearn) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from tslearn) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from tslearn) (1.2.2)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from tslearn) (0.56.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from tslearn) (1.2.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->tslearn) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->tslearn) (67.7.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->tslearn) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
        "from tslearn.clustering import TimeSeriesKMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "from tables.file import defaultdict\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "H7w02ROMaCba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_stock_data(path_data,path_compagnie):\n",
        "  compagnie=pd.read_csv(path_compagnie)\n",
        "  stocks = pd.read_csv(path_data)\n",
        "  stocks['Date']=pd.to_datetime(stocks['Date'], yearfirst = True)\n",
        "  names=set(stocks['Symbol'].tolist())\n",
        "  names=dict.fromkeys(names, 0)\n",
        "  dataset_global= defaultdict(lambda:0)\n",
        "  for stock in sorted(names):\n",
        "    dataset_global[stock]=stocks[stocks['Symbol']==stock].loc[:,[\"Date\",\"Adj Close\",\"Close\",\"High\",\"Low\",\"Open\"]]\n",
        "\n",
        "  final_names=defaultdict(lambda:0)\n",
        "  for stock in sorted(names):\n",
        "    nbr=dataset_global[stock]['Adj Close'].isnull().sum()\n",
        "    if nbr==0:\n",
        "      final_names[stock]=0\n",
        "\n",
        "  market={\"NMS\":[],\"NYQ\":[],'NGM':[],'BTS':[]}\n",
        "\n",
        "  # delete stock that I can't identify exchange market\n",
        "  delt=0\n",
        "  for stock in sorted(final_names):\n",
        "    if compagnie[compagnie.Symbol==stock]['Exchange'].empty:\n",
        "      final_names.pop(stock)\n",
        "      delt+=1\n",
        "    else:\n",
        "      market[(compagnie[compagnie.Symbol==stock]['Exchange'].values)[0]].append(stock)\n",
        "\n",
        "  dataset= defaultdict(lambda:0)\n",
        "\n",
        "  for stock in sorted(final_names):\n",
        "    dataset[stock]=pd.concat([dataset_global[stock].loc[:,['Date',\"Adj Close\",\"Close\",\"High\",\"Low\",\"Open\"]],((pd.DataFrame(dataset_global[stock].loc[:,\"Adj Close\"]).pct_change(periods=1)+1)*100).rename(columns={'Adj Close':'Adj_pct'})],axis=1, join='inner')\n",
        "\n",
        "  return (dataset,market,final_names)"
      ],
      "metadata": {
        "id": "f9a-qfRxAENl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_feature(data,final_names):\n",
        "  mov_avg_close_5=defaultdict(lambda:0)\n",
        "  mov_avg_close_12=defaultdict(lambda:0)\n",
        "  mov_avg_close_26=defaultdict(lambda:0)\n",
        "  mov_avg_close_30=defaultdict(lambda:0)\n",
        "  mov_avg_close_50=defaultdict(lambda:0)\n",
        "\n",
        "  for stock in sorted(final_names):\n",
        "    mov_avg_close_5[stock]=pd.concat([data[stock]['Date'],data[stock].loc[:,'Adj Close':'Open'].rolling(5).mean().rename(columns={'Adj Close':'Adj_mvg5','Close':'Close_mvg5','High':'High_mvg5','Low':'Low_mvg5','Open':'Open_mvg5'})],axis=1, join='inner')\n",
        "    mov_avg_close_12[stock]=pd.concat([data[stock]['Date'],data[stock].loc[:,'Adj Close':'Open'].rolling(12).mean().rename(columns={'Adj Close':'Adj_mvg12','Close':'Close_mvg12','High':'High_mvg12','Low':'Low_mvg12','Open':'Open_mvg12'})],axis=1, join='inner')\n",
        "    mov_avg_close_26[stock]=pd.concat([data[stock]['Date'],data[stock].loc[:,'Adj Close':'Open'].rolling(26).mean().rename(columns={'Adj Close':'Adj_mvg26','Close':'Close_mvg26','High':'High_mvg26','Low':'Low_mvg26','Open':'Open_mvg26'})],axis=1, join='inner')\n",
        "    mov_avg_close_30[stock]=pd.concat([data[stock]['Date'],data[stock].loc[:,'Adj Close':'Open'].rolling(30).mean().rename(columns={'Adj Close':'Adj_mvg30','Close':'Close_mvg30','High':'High_mvg30','Low':'Low_mvg30','Open':'Open_mvg30'})],axis=1, join='inner')\n",
        "    mov_avg_close_50[stock]=pd.concat([data[stock]['Date'],data[stock].loc[:,'Adj Close':'Open'].rolling(50).mean().rename(columns={'Adj Close':'Adj_mvg50','Close':'Close_mvg50','High':'High_mvg50','Low':'Low_mvg50','Open':'Open_mvg50'})],axis=1, join='inner')\n",
        "\n",
        "  for stock in sorted(final_names):\n",
        "    data[stock]=data[stock].merge(mov_avg_close_5[stock],how='inner', on='Date')\n",
        "    data[stock]=data[stock].merge(mov_avg_close_12[stock],how='inner', on='Date')\n",
        "    data[stock]=data[stock].merge(mov_avg_close_26[stock],how='inner', on='Date')\n",
        "    data[stock]=data[stock].merge(mov_avg_close_30[stock],how='inner', on='Date')\n",
        "    data[stock]=data[stock].merge(mov_avg_close_50[stock],how='inner', on='Date')\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "jiLyfcTMMuW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stock_rank(data,market_list,market_name):\n",
        "  stock_return=data[sorted(market_list[market_name])[0]].loc[:,['Date','Adj_pct']].rename(columns={'Adj_pct':sorted(market_list[market_name])[0]+\"_rt\"})\n",
        "  data[sorted(market_list[market_name])[0]].drop(columns=['Adj_pct'],inplace=True)\n",
        "  for stock in sorted(market_list[market_name])[1:]:\n",
        "    stock_return=stock_return.merge(data[stock].loc[:,['Date','Adj_pct']].rename(columns={'Adj_pct':stock+\"_rt\"}),how='inner',on='Date')\n",
        "    data[stock].drop(columns=['Adj_pct'],inplace=True)\n",
        "\n",
        "  rank_stock=pd.concat([stock_return[\"Date\"].iloc[1:],stock_return.iloc[1:,:].rank(axis=1,method=\"min\")],axis=1,join=\"inner\")\n",
        "  return (rank_stock,stock_return)\n"
      ],
      "metadata": {
        "id": "mD6yQ2hCMyuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def final_data(data,market_list,market_name,rank_stock):\n",
        "  torch_data=torch.from_numpy(data[sorted(market_list[market_name])[0]].iloc[1:,:].merge(rank_stock[0].loc[:,['Date',sorted(market_list[market_name])[0]+'_rt']],how='inner', on='Date').iloc[:,1:].values.T).unsqueeze(0)\n",
        "  for stock in sorted(market_list[market_name][1:]):\n",
        "    torch_data= torch.cat([torch_data,torch.from_numpy(data[stock].iloc[1:,:].merge(rank_stock[0].loc[:,['Date',stock+'_rt']],how='inner', on='Date').iloc[:,1:].values.T).unsqueeze(0)],0)\n",
        "  stock_rt=(rank_stock[1].iloc[1:,1:].values).T\n",
        "  return (torch_data, stock_rt)"
      ],
      "metadata": {
        "id": "WEWDmlpfIZe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nBfe6rDZyxO"
      },
      "outputs": [],
      "source": [
        "def split_datasets_for_training(data,r_data, window_size,scale=True):\n",
        "  _l = data.shape[-1]\n",
        "  Xs = []\n",
        "  Ys = []\n",
        "  Rs=[]\n",
        "  for i in range(48, (_l - window_size-1)):\n",
        "    Xs.append(data[:,:-1,i:i+window_size])\n",
        "    # Ys.append(data[:,-1,i+window_size+1])\n",
        "    Ys.append(r_data[:,i+window_size+1])\n",
        "    x=r_data[:,i:i+window_size]\n",
        "    # y=r_data[:,i+window_size+1]\n",
        "    Rs.append(x)\n",
        "\n",
        "  # tr_x, ts_x, tr_y, ts_y,tr_r,ts_r,tr_s,ts_s = train_test_split(Xs, Ys,Rs,y,shuffle=False)\n",
        "  tr_x, ts_x, tr_y, ts_y,tr_r,ts_r = train_test_split(Xs,Ys,Rs,shuffle=False)\n",
        "\n",
        "  # return 1\n",
        "  return  (tr_x, ts_x, tr_y, ts_y,tr_r,ts_r)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class build_cluster():\n",
        "  '''\n",
        "  'silhouette','calinski_harabasz',distortion\n",
        "\n",
        "  '''\n",
        "  def __init__(self, metric: str,KElbowmetric:str, iteration: int, num_clus:int=0) :\n",
        "    super(build_cluster, self).__init__()\n",
        "    self.Kmeans_metric=metric\n",
        "    self.metric=KElbowmetric\n",
        "    self.num_cluster=num_clus\n",
        "    self.iter=iteration\n",
        "    self.model= TimeSeriesKMeans(metric= self.Kmeans_metric, max_iter=self.iter)\n",
        "\n",
        "  def predict(self,data,k=(2,10),best=True):\n",
        "    # visualizer = KElbowVisualizer(self.model, k=k,metric=self.metric, timings= True)\n",
        "    # visualizer.fit(np.array(data))\n",
        "    # if best and visualizer.elbow_value_ !=None:\n",
        "    #   self.num_cluster= visualizer.elbow_value_\n",
        "    self.num_cluster=8\n",
        "    model=TimeSeriesKMeans(n_clusters=self.num_cluster,metric= self.Kmeans_metric, max_iter=self.iter).fit(np.array(data))\n",
        "    label=model.predict(np.array(data))\n",
        "    return label"
      ],
      "metadata": {
        "id": "ZwhUMUsce9Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Adjacency_Matrix:\n",
        "    def __init__(self, market_name):\n",
        "        # self.data_path = data_path\n",
        "        # self.date_format = '%Y-%m-%d'\n",
        "        self.market_name = market_name\n",
        "\n",
        "    def generate_stock_relation(self, market_dict,\n",
        "                                 pred_clusters,lap=False):\n",
        "\n",
        "        selected_stock=sorted(market_dict[self.market_name])\n",
        "        stock_index = {}\n",
        "\n",
        "        for index, ticker in enumerate(selected_stock):\n",
        "            stock_index[ticker] = index\n",
        "\n",
        "        valid_label_count = 0\n",
        "        valid_label_index = {}\n",
        "\n",
        "        for label in np.unique(pred_clusters):\n",
        "\n",
        "\n",
        "          if np.where(pred_clusters==label)[0].shape[0]>1:\n",
        "              valid_label_index[str(label)] = valid_label_count\n",
        "              valid_label_count += 1\n",
        "\n",
        "        one_hot_label_embedding = np.identity(valid_label_count,\n",
        "                                                 dtype=float)\n",
        "\n",
        "        stock_relation_embedding = np.zeros(\n",
        "            [len(selected_stock), len(selected_stock),\n",
        "             valid_label_count], dtype=float)\n",
        "\n",
        "\n",
        "\n",
        "        for label in valid_label_index.keys():\n",
        "            cur_ind_stock = np.where(pred_clusters==int(label))\n",
        "            if cur_ind_stock[0].shape[0] <= 1:\n",
        "                print('shit label:', label)\n",
        "                continue\n",
        "            ind_ind = valid_label_index[label]\n",
        "\n",
        "\n",
        "            for i in range(cur_ind_stock[0].shape[0]):\n",
        "\n",
        "              left_tic_ind = cur_ind_stock[0][i]\n",
        "\n",
        "              stock_relation_embedding[left_tic_ind][left_tic_ind] = copy.copy(one_hot_label_embedding[ind_ind])\n",
        "\n",
        "\n",
        "\n",
        "              for j in range(i + 1, cur_ind_stock[0].shape[0]):\n",
        "\n",
        "                right_tic_ind = cur_ind_stock[0][j]\n",
        "                stock_relation_embedding[left_tic_ind][right_tic_ind] = copy.copy(one_hot_label_embedding[ind_ind])\n",
        "                stock_relation_embedding[right_tic_ind][left_tic_ind] = copy.copy(one_hot_label_embedding[ind_ind])\n",
        "\n",
        "\n",
        "        adj=torch.from_numpy(np.sum(stock_relation_embedding,axis=2)).to_sparse()\n",
        "\n",
        "        # compute the weight of edges\n",
        "\n",
        "\n",
        "        return adj\n"
      ],
      "metadata": {
        "id": "5UqaqcpsdEAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_edges(adj):\n",
        "  ''' This function is used to get edges of the adjacency matrix\n",
        "  '''\n",
        "  return adj.indices()\n",
        "\n",
        "  # adj_edges=[adj.indices()[:,i].tolist() for i in range(adj.indices().shape[-1])]\n",
        "  # return torch.tensor(adj_edges).t().contiguous()"
      ],
      "metadata": {
        "id": "jlMdtl3qajFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_data(x,y,predict_cluster,market_dict,market_name,batch_size=32,seq_len=60):\n",
        "    # Define the batch size and the sequence length\n",
        "    batch_adj=[]\n",
        "    batch_x=[]\n",
        "    batch_y=[]\n",
        "    Adj=Adjacency_Matrix(market_name)\n",
        "\n",
        "    # Define the number of nodes and the number of edges\n",
        "    num_nodes = len(market_dict[market_name])\n",
        "    # num_edges = 2\n",
        "\n",
        "    num_batch=(len(x)//batch_size)\n",
        "    for k in range(num_batch):\n",
        "\n",
        "      batch_x.append(torch.cat([x[ind].unsqueeze(0).float()  for ind in range(k*batch_size,(k+1)*batch_size)],axis=0))\n",
        "      # batch_y.append(torch.cat([y[ind].unsqueeze(0).float()  for ind in range(k*batch_size,(k+1)*batch_size)],axis=0))\n",
        "      batch_y.append(torch.cat([torch.from_numpy(y[ind]).unsqueeze(0).float()  for ind in range(k*batch_size,(k+1)*batch_size)],axis=0))\n",
        "\n",
        "\n",
        "\n",
        "      cluster=predict_cluster[k*batch_size:(k+1)*batch_size]\n",
        "\n",
        "      # Initialize the empty adjacency matrix\n",
        "      # adj = torch.zeros(batch_size, seq_len, num_nodes,num_nodes)\n",
        "      adj = torch.zeros(batch_size, num_nodes,num_nodes)\n",
        "\n",
        "\n",
        "      # Fill in the adjacency matrix for each time step in each batch\n",
        "      for i in range(batch_size):\n",
        "          # Define the adjacency matrix for a single time step\n",
        "          adj_t = Adj.generate_stock_relation(market_dict,cluster[i]).to_dense()\n",
        "          A_hat= adj_t+torch.eye(adj_t.shape[0])\n",
        "          D=torch.linalg.inv(torch.diag(torch.sum(A_hat,axis=1)))**(0.5)\n",
        "          adj_norm= torch.mm(torch.mm(D,A_hat),D)\n",
        "          adj[i,:, :] = copy.copy(adj_norm)\n",
        "          # for j in range(seq_len):\n",
        "\n",
        "              # adj_t = adj_t   # Add a new dimension for the\n",
        "              # adj_t = adj_t.unsqueeze(0).unsqueeze(0).repeat(1, 1, 1, 1, num_edges)   # Add a new dimension for the edges\n",
        "              # adj[i, j, :, :] = copy.copy(adj_t)\n",
        "      batch_adj.append(adj)\n",
        "    return batch_x,batch_y,batch_adj\n",
        "\n"
      ],
      "metadata": {
        "id": "ccvmlQ7urltV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ndcg_score(targets, scores, k=127):\n",
        "\n",
        "    scores_predict=torch.argsort(torch.argsort(scores,descending=True, dim=1),dim=1)\n",
        "    score_real=torch.argsort(torch.argsort(targets,descending=True, dim=1),dim=1)\n",
        "\n",
        "    relevance=torch.where(scores_predict<50,1,0) # predicted relevance\n",
        "    relevance_real=torch.where(score_real<50,1,0) #real relevance\n",
        "\n",
        "    _, indices = torch.topk(targets, k, dim=1, largest=True, sorted=True)# find the index of the top k relevant stock\n",
        "\n",
        "    sorted_targets = torch.gather(relevance, 1, indices)# use that index to get their predicted relevance score\n",
        "    discounts = torch.log2(torch.arange(2, k + 2).float())# compute de discount\n",
        "    gains = (2 ** sorted_targets - 1) / discounts\n",
        "\n",
        "    ideal_sorted_targets = torch.gather(relevance_real, 1, indices)# use the index of top k relevant to get their true relevance score\n",
        "    ideal_gains = (2 ** ideal_sorted_targets - 1) / discounts\n",
        "\n",
        "    dcg = torch.sum(gains, dim=1)\n",
        "    ideal_dcg = torch.sum(ideal_gains, dim=1)\n",
        "\n",
        "    ndcg = torch.mean(dcg/ideal_dcg)\n",
        "    return ndcg"
      ],
      "metadata": {
        "id": "R0PXdEBhzwdF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}